output_path: /network/tmp1/schmidtv/yb_runs/test_v1
# -------------------
# -----  Tasks  -----
# -------------------
tasks: [d, s, m]

# ----------------
# ----- Data -----
# ----------------
data:
  files: # if one is not none it will override the dirs location
    base: /path/to/data
    train:
      r: train_r.json
      s: train_s.json
    val:
      r: val_r.json
      s: val_s.json

  loaders:
    batch_size: 2
    simclr_batch_size: 256
    shuffle: true
    num_workers: 8
  transforms:
    - name: hflip
      ignore: false
      p: 0.5
    - name: resize
      ignore: false
      new_size: 256
    - name: crop
      ignore: false
      height: 224
      width: 224
    - name: resize # ? this or change generator's output? Or resize larger then crop to 256?
      ignore: false
      new_size: 256

# ---------------------
# ----- Generator -----
# ---------------------
gen:
  opt:
    optimizer: ExtraAdam # one in [Adam, ExtraAdam] default: Adam #TODO Test speed/performance difference w/ Adam
    beta1: 0.9
    lr: 0.0005
    lr_policy: step # constant or step ; if step, specify step_size and gamma
    lr_step_size: 30 # for linear decay : period of learning rate decay (epochs)
    lr_gamma: 0.5 # Multiplicative factor of learning rate decay
  default:
    &default-gen # default parameters for the generator (encoder and decoders)
    activ: lrelu # activation function [relu/lrelu/prelu/selu/tanh]
    init_gain: 0.2
    init_type: kaiming
    n_res: 1 # number of residual blocks before upsampling
    n_downsample: &n_downsample 3 # number of downsampling layers in encoder | dim 32 + down 3 => z = 256 x 32 x 32
    n_upsample: *n_downsample # upsampling in spade decoder ; should match encoder.n_downsample
    pad_type: reflect # padding type [zero/reflect]
    res_dim: 2048 # Resblock number of channels (=latent space's), should be 2048 if using deeplabv2, 256 otherwise
    res_norm: spectral # ResBlock normalization ; one of {"batch", "instance", "layer", "adain", "spectral", "none"}
    proj_dim: 64 # Dim of projection from latent space
  encoder: # specific params for the encoder
    <<: *default-gen
    dim: 32
    architecture: deeplabv2 # [deeplabv2 (res_dim = 2048) | base (res_dim = 256)]
    input_dim: 3 # input number of channels
    n_res: 1 # number of residual blocks in content encoder/decoder
    norm: spectral # ConvBlock normalization ; one of {"batch", "instance", "layer", "adain", "spectral", "none"}

  #! Don't change!!!
  deeplabv2:
    nblocks: [3, 4, 23, 3]
    use_pretrained: True
    pretrained_model: "/network/tmp1/ccai/data/omnigan/pretrained_models/DeepLab_resnet_pretrained_imagenet.pth"

  simclr:
    is_trained: False # Temporary, remove when trained
    use_pretrained: True
    # pretrained_model not existing yet!
    pretrained_model: "/network/tmp1/ccai/data/omnigan/pretrained_models/SimCLR_pretrained.pth"
    loss:
      temp: 0.5
      use_cosine_similarity: True
    input_size: 256 # must be the same as the resize of transforms?
    output_size: 256
    colorization_s: 1 # parameter between 0 and 1 defining how much we want colorization

  d: # specific params for the depth estimation decoder
    <<: *default-gen
    output_dim: 1
  s: # specific params for the semantic segmentation decoder
    <<: *default-gen
    num_classes: 19
    output_dim: 19
  t: # specific params for the translation SPADE decoder
    <<: *default-gen
    output_dim: 3 # output dimension
    pad_type: zero # padding type [zero/reflect]
    use_spade: True # whether to condition the decoding on other inferences, Ã  la SPADE or not
    spade_kernel_size: 3 # kernel size within SPADE norm layers
    spade_param_free_norm: instance # what param-free normalization to apply in SPADE normalization
    spade_use_spectral_norm: true
    spade_n_up: *n_downsample # number of upsampling layers in the translation decoder is equal to number of downsamplings in the encoder.  output's h and w are z's h and w x 2^spade_num_upsampling_layers | z:32 and spade_n_up:4 => output 512
    use_bit_conditioning: True
  m: # specific params for the mask-generation decoder
    <<: *default-gen
    output_dim: 1

# -------------------------
# ----- Discriminator -----
# -------------------------
dis:
  soft_shift: 0.2 # label smoothing: real in U(1-soft_shift, 1), fake in U(0, soft_shift) # ! one-sided label smoothing
  flip_prob: 0.05 # label flipping
  opt:
    optimizer: ExtraAdam # one in [Adam, ExtraAdam] default: Adam
    beta1: 0.5
    lr: 0.0005
    lr_policy: step # constant or step ; if step, specify step_size and gamma
    lr_step_size: 30 # for linear decay
    lr_gamma: 0.5
  default:
    &default-dis # default setting for discriminators (there are 4 of them for rn rf sn sf)
    input_nc: 3
    ndf: 64
    n_layers: 3
    norm: instance
    init_type: kaiming
    init_gain: 0.2
    use_sigmoid: false

  t:
    <<: *default-dis
  m:
    <<: *default-dis

# -------------------------------
# -----  Domain Classifier  -----
# -------------------------------
classifier:
  opt:
    optimizer: ExtraAdam # one in [Adam, ExtraAdam] default: Adam
    beta1: 0.5
    lr: 0.0005
    lr_policy: step # constant or step ; if step, specify step_size and gamma
    lr_step_size: 30 # for linear decay
    lr_gamma: 0.5
  loss: cross_entropy #Loss can be l1, l2, cross_entropy.  default cross_entropy
  layers: [100, 100, 20, 20, 4] # number of units per hidden layer ; las number is output_dim
  dropout: 0.4 # probability of being set to 0
  init_type: kaiming
  init_gain: 0.2
  proj_dim: 128 #Dim of projection from latent space

# ------------------------
# ----- Train Params -----
# ------------------------
train:
  epochs: 100000000
  representational_training: True
  representation_steps: 10000 # for how many steps would the representation be trained before we train the translation
  freeze_representation: False # whether or not to backprop into the representation when doing the translation
  lambdas: # scaling factors in the total loss
    G:
      d: 1
      s: 1
      m:
        main: 1 # Main prediction loss, i.e. GAN or BCE
        tv: 1 # Total variational loss (for smoothing)
      t:
        auto: 1 # auto-encoding, reconstruction
        cycle: 1 # cycle consistency
        gan: 1 # gan loss
        sm: 1 # semantic matching
        dm: 1 # depth matching
      simclr: 1
      classifier: 1
    C: 1
  log_level: 2 # 0: no log, 1: only aggregated losses, >1 detailed losses
  save_n_epochs: 1 # Save model every n epochs
  resume: false # Load latest_ckpt.pth checkpoint from `output_path` #TODO Make this path of checkpoint to load

# -----------------------------
# ----- Validation Params -----
# -----------------------------
val:
  store_images: false # write to disk on top of comet logging

# -----------------------------
# ----- Comet Params ----------
# -----------------------------
comet:
  display_size: 5
