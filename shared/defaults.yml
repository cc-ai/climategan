output_path: /network/tmp1/schmidtv/yb_runs/test_v1
# -------------------
# -----  Tasks  -----
# -------------------
tasks: [a, d, h, s, t, w]

# ----------------
# ----- Data -----
# ----------------
data:
  files: # if one is not none it will override the dirs location
    base: /path/to/data
    train:
      rf: train_rf.json
      rn: train_rn.json
      sf: train_sf.json
      sn: train_sn.json
    val:
      rf: val_rf.json
      rn: val_rn.json
      sf: val_sf.json
      sn: val_sn.json
  loaders:
    batch_size: 2
    shuffle: false
    num_workers: 8
  transforms:
    - name: hflip
      ignore: false
      p: 0.5
    - name: resize
      ignore: false
      new_size: 256
    - name: crop
      ignore: false
      height: 224
      width: 224
    - name: resize
      ignore: false
      new_size: 256

# ---------------------
# ----- Generator -----
# ---------------------
gen:
  opt:
    optimizer: ExtraAdam # one in [Adam, ExtraAdam] default: Adam
    lr_policy: false
    epoch_count: null # for linear decay
    niter: null # for linear or cosine decay
    niter_decay: null # for linear decay
    lr: 0.0005
    beta1: 0.9
  encoder:
    n_blocks: 4 # number of UNet blocks
    bilinear: True # False to use transpose convs instead of bilinear upsampling
    base_channels: 64 # number of channels to double from in the Unet
  default:
    input_nc: 3
    output_nc: 3
    ngf: 64
    norm_layer: batch
    dropout: 0.2
    n_blocks: 6
    padding_type: reflect
    init_type: kaiming
    init_gain: 0.2
    lambda_rec: 1
    lambda_idt: 1
  s:
    num_classes: 19
# Set gen.A.init_gain to overwrite this parameter for A only

# -------------------------
# ----- Discriminator -----
# -------------------------
dis:
  opt:
    optimizer: ExtraAdam
    lr_policy: false
    epoch_count: null # for linear decay
    niter: null # for linear or cosine decay
    niter_decay: null # for linear decay
    lr: 0.0005
    beta1: 0.5
  default: # default setting for discriminators (there are 4 of them for rn rf sn sf)
    input_nc: 3
    ndf: 64
    n_layers: 3
    norm: instance
    init_type: kaiming
    init_gain: 0.2
    use_sigmoid: false

# -------------------------------
# -----  Domain Classifier  -----
# -------------------------------
classifier:
  opt:
    optimizer: ExtraAdam
    lr: 0.0005
    beta1: 0.5
  loss: cross_entropy #Loss can be l1, l2, cross_entropy.  default cross_entropy
  layers: [100, 100, 20, 20, 4] # number of units per hidden layer ; las number is output_dim
  dropout: 0.4 # probability of being set to 0
  init_type: kaiming
  init_gain: 0.2

# ------------------------
# ----- Train Params -----
# ------------------------
train:
  epochs: 1000
  representational_training: False
  representation_steps: 0 # for how many steps would the representation be trained before we train the translation
  freeze_representation: False # whether or not to backprop into the representation when doing the translation
  lambdas: # scaling factors in the total loss
    default: 1 # default values for all tasks
    G:
      auto: # auto-encoder loss coefficients
        a: 1
        t: 1
      gan: # auto-encoder loss coefficients
        a: 1
        t: 1
      cycle: # auto-encoder loss coefficients
        a: 1
        t: 1
      classifier: 1
    C: 1
  log_level: 1 # 0: no log, 1: only aggregated losses, >1 detailed losses

# -----------------------------
# ----- Validation Params -----
# -----------------------------
val:
  max_log_images: 15
  every_n_steps: 1000
  store_images: false
  infer_rec: true
  infer_idt: true # order: real, translated, rec, idt
